{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score\n",
    "import featuretools as ft\n",
    "import time\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "from sklearn.feature_selection import RFE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RandomForestRegressor_FE\n",
      "_____________\n",
      "Modelling:Office_Mark\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 72 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 92 features.\n",
      "Fitting estimator with 82 features.\n",
      "Fitting estimator with 72 features.\n",
      "[5 6 7] (2208, 71) (2208,) [ 8  9 10] (2208, 71) (2208,)\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 73 features.\n",
      "[ 5  6  7  8  9 10] (4416, 71) (4416,) [11 12  1] (2209, 71) (2209,)\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 73 features.\n",
      "[ 5  6  7  8  9 10 11 12  1] (6625, 71) (6625,) [2 3 4] (2135, 71) (2135,)\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 73 features.\n",
      "[ 5  6  7  9 10 11  1  2  3] (6552, 71) (6552,) [ 8 12  4] (2208, 71) (2208,)\n",
      "Time per building after FE and FS:00:27:12\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Read meta data\n",
    "meta = pd.read_csv('/Users/t.wang/Desktop/Dissertation/Python/input/meta_open.csv', \n",
    "                   index_col='uid', parse_dates=['dataend','datastart'], dayfirst=True)#The data will be messed up withou specifying dayfirst\n",
    "\n",
    "\n",
    "# Read energy data\n",
    "temporal = pd.read_csv('/Users/t.wang/Desktop/Dissertation/Python/input/temp_open_utc_complete.csv', \n",
    "                   index_col='timestamp', parse_dates=True)#.tz_localize('utc')\n",
    "\n",
    "def loopModels_and_Metrics(ml_Models_names, ml_Models, weatherPoints, n_timeSeriesSplits, \n",
    "                           buildingNames, agg_primitives, trans_primitives, varianceThreshold, SelectPercentile_num,\n",
    "                           SelectPercentile_func, RFE_step):  \n",
    "    print('\\n\\n' + ml_Models_names + '\\n_____________')\n",
    "    buildingindex = 0\n",
    "    for single_building in buildingNames:\n",
    "        buildingindex+=1\n",
    "        print('Modelling:' + single_building)\n",
    "        \n",
    "        # Read energy data for each given buildingname\n",
    "        single_timezone = meta.T[single_building].timezone\n",
    "        startdate = meta.T[single_building].datastart\n",
    "        enddate = meta.T[single_building].dataend\n",
    "        single_building_energy = temporal[single_building].tz_convert(single_timezone).truncate(before = startdate, \n",
    "                                                            after = enddate)#.fillna(method='bfill').fillna(method='ffill')\n",
    "                                                            # single_building_energy, some missing data\n",
    "\n",
    "\n",
    "        # Get weather data for given building\n",
    "        weatherfile_name = meta.T[single_building].newweatherfilename\n",
    "        weather_data = pd.read_csv(os.path.join('/Users/t.wang/Desktop/Dissertation/Python/input/',\n",
    "                                                weatherfile_name),index_col='timestamp', parse_dates=True, na_values='-9999')\n",
    "        weather_data = weather_data.tz_localize(single_timezone, ambiguous = 'infer')\n",
    "        weather_point_list=[]\n",
    "        for point in weatherPoints:\n",
    "            point_data = weather_data[[point]]\n",
    "            weather_point_list.append(point_data)\n",
    "            all_weather_point = pd.concat(weather_point_list,axis=1) #axis=1, rowwise concat\n",
    "            all_weather_point = all_weather_point[~all_weather_point.index.duplicated()]#To get rid of duplicated index\n",
    "            all_weather_point = all_weather_point.reindex(pd.DatetimeIndex(start = all_weather_point.index[0], \n",
    "                                                                           periods=len(single_building_energy), \n",
    "                                                                           freq='H')).fillna(method='ffill').fillna(method='bfill')\n",
    "#             in some cases, there are more than 1 data in the same hour, creating more than 8760 points\n",
    "#             to make them consistent, take the first index minuits, based on the number of energy data,\n",
    "#             transform them into hourly data. Then we get the same number of energy data (mostly8760)\n",
    "#             DatatimeIndex them, reindex then is able to match and select those hour with the minuites\n",
    "#             same as first index, regulating the data to be consistent with number of energy points, get\n",
    "#             rid of the repeated weather data in the same hour.\n",
    "    \n",
    "        # Get schedule data for given building\n",
    "        schedule_name = meta.T[single_building].annualschedule\n",
    "        schedule_data = pd.read_csv(os.path.join('/Users/t.wang/Desktop/Dissertation/Python/input/',\n",
    "                                                schedule_name),index_col=0, header=None, parse_dates=True)\n",
    "        schedule_data = schedule_data.tz_localize(single_timezone, ambiguous = 'infer')\n",
    "        schedule_data.columns = ['seasonal']\n",
    "        schedule_data = schedule_data.reindex(pd.DatetimeIndex(start = schedule_data.index[0], periods=len(single_building_energy), \n",
    "                                                               freq='H')).fillna(method='ffill').fillna(method='bfill')\n",
    "#         same trick is applied to selecting schedule data\n",
    "\n",
    "        # Create TimeSeriesSplit\n",
    "        # get the month number for splitting\n",
    "        months = np.array(single_building_energy.index.month.unique())\n",
    "        tscv = TimeSeriesSplit(n_splits=n_timeSeriesSplits)\n",
    "        train_test_list = []\n",
    "        for train_index, test_index in tscv.split(months):\n",
    "            months_train, months_test = months[train_index], months[test_index]\n",
    "            train_test_list.append([months_train, months_test])\n",
    "        # Add the 'every-four-month' version, 5th month is duplicated in Clayton's notebook\n",
    "        train_test_list.append([np.concatenate([months[0:3],months[4:7],months[8:11]]),\n",
    "                                             np.array([months[3],months[7],months[11]])])\n",
    "        \n",
    "        index = 0 #index for each TimeSeries cv\n",
    "        for train_index, test_index in train_test_list: #get rid of the 'array', extract the numeric months from the list\n",
    "        #     print(train_index, test_index)\n",
    "            months_for_train = train_index\n",
    "            months_for_test = test_index\n",
    "\n",
    "            # Create features and labels under last 'for' loop such that all TimeSeriesSplit could be implenmented\n",
    "            def get_features_and_labels(train_or_test):\n",
    "                nonlocal single_building_energy #nonlocal means: \"look for this variable in the outer scope\"\n",
    "                nonlocal all_weather_point\n",
    "                nonlocal schedule_data\n",
    "                single_building_energy_n = single_building_energy[single_building_energy.index.month.isin(train_or_test)]\n",
    "                all_weather_point_n = all_weather_point[all_weather_point.index.month.isin(train_or_test)]\n",
    "                schedule_data_n = schedule_data[schedule_data.index.month.isin(train_or_test)]\n",
    "                #rename _n is required, otherwise the function will run on top of incomplete dataset after one running(after traindata, testdata disappeared)\n",
    "                \n",
    "                \n",
    "                \n",
    "                '''Feature Engineering'''\n",
    "#                 features = pd.merge(pd.get_dummies(single_building_energy_n.index.hour),\n",
    "#                                      pd.get_dummies(single_building_energy_n.index.dayofweek), right_index=True, left_index=True)\n",
    "# #                 features = pd.merge(features, pd.get_dummies(schedule_data_n.reset_index(drop=True)), right_index=True, left_index=True)\n",
    "#                 features = pd.merge(features, schedule_data_n.reset_index(drop=True), right_index=True, left_index=True)\n",
    "#                 features['seasonal_num'] = features.seasonal.map({'Break':0, 'Regular':1, 'Holiday':2, 'Summer':3})\n",
    "#                 features = features.drop('seasonal', axis=1)\n",
    "#         #     instead of get dummies in schedule data, map schedule strings to numbers in just one column helps to solve the inconsistency of schedules\n",
    "#         #     may result in information loss\n",
    "#                 features = pd.concat([features, all_weather_point_n.reset_index(drop=True)], axis=1) \n",
    "#                 #.reset_index(drop=True) to get rid of the time index, otherwise two sets data will stratify\n",
    "#                 features = features.fillna(method='ffill').fillna(method='bfill')\n",
    "#                 features = np.array(features)\n",
    "#                 labels = single_building_energy_n.values\n",
    "#                 return features, labels\n",
    "\n",
    "                features = pd.merge(pd.DataFrame(single_building_energy_n.index.tz_localize(None)), \n",
    "                    schedule_data_n.reset_index(drop=True), right_index=True, left_index=True)#remove the time zone information\n",
    "                #Map the schedule, otherwise the TimeSplits will not be able to capture all schedules, resulting in inconsistency of traning/test feature dimensions\n",
    "                features['seasonal_num'] = features.seasonal.map({'Break':0, 'Regular':1, 'Holiday':2, 'Summer':3})\n",
    "                features = features.drop('seasonal', axis=1)\n",
    "                features = pd.concat([features, all_weather_point_n.reset_index(drop=True)], axis=1) #.reset_index(drop=True) to get rid of the time index, otherwise two sets data will stratify\n",
    "#                 features = features.fillna(method='ffill').fillna(method='bfill')\n",
    "                # features = np.array(features)\n",
    "                labels = single_building_energy_n.values\n",
    "                '''FeatureTool'''\n",
    "                es = ft.EntitySet(id = 'buildingFeatures') #create Entity set\n",
    "                # create an entity from feature table, unique index is created\n",
    "                es = es.entity_from_dataframe(entity_id='featureData', dataframe=features,\n",
    "                              make_index=True, index='feature_id', time_index = 'timestamp')\n",
    "                features, feature_names = ft.dfs(entityset = es, target_entity = 'featureData', max_depth = 2\n",
    "                                ,agg_primitives = agg_primitives,\n",
    "                                trans_primitives = trans_primitives, verbose = False)\n",
    "                # one hot encoding for categorical data\n",
    "                features_enc, feature_names_enc = ft.encode_features(features, feature_names)\n",
    "                # Replace infinity number arising after feature generation\n",
    "                features_enc = features_enc.replace(np.inf, '9999')\n",
    "                features_enc = features_enc.replace(-np.inf, '-9999')\n",
    "                features_enc = features_enc.replace([np.nan,''],0)\n",
    "              \n",
    "                '''Feature Selection'''\n",
    "                '''VarianceThreshold is not dertermistic, yielding different train/test numbers, hence inconsistency'''\n",
    "#                 Filter methods - Removing features with low variance - SKlearn\n",
    "                y = labels\n",
    "                X = features_enc\n",
    "#                 sel = VarianceThreshold(threshold=(varianceThreshold * (1 - varianceThreshold)))\n",
    "#                 X_Variance = sel.fit_transform(X)\n",
    "            \n",
    "#                 Filter methods - Univariate feature selection - SKlearn    \n",
    "                X_Variance_Percentile = SelectPercentile(score_func=SelectPercentile_func, percentile=SelectPercentile_num).fit_transform(X,y)\n",
    "    \n",
    "#                 Wrapper methods - Recursive feature elimination -SKlearn\n",
    "                Wrapper_model = ml_Models\n",
    "                rfe = RFE(estimator=Wrapper_model, step=RFE_step,verbose=True)\n",
    "                rfe = rfe.fit(X_Variance_Percentile, y)\n",
    "                X_Variance_Percentile_RFE = rfe.transform(X_Variance_Percentile)\n",
    "                        \n",
    "                return X_Variance_Percentile_RFE, y\n",
    "\n",
    "\n",
    "            # test on model and calculate errors\n",
    "            X_train, y_train = get_features_and_labels(train_or_test=months_for_train)\n",
    "            X_test, y_test = get_features_and_labels(train_or_test=months_for_test)\n",
    "            print(months_for_train, X_train.shape, y_train.shape, months_for_test, X_test.shape, y_test.shape)\n",
    "#             compare = pd.concat([pd.DataFrame(X_train), pd.DataFrame(X_test)], axis=1) \n",
    "#             pd.DataFrame(X_train).to_csv('/Users/t.wang/Desktop/' + 'X_train' +'.csv', index=False)\n",
    "#             pd.DataFrame(X_test).to_csv('/Users/t.wang/Desktop/' + 'X_test' +'.csv', index=False)\n",
    "            ml_Models.fit(X_train, y_train)\n",
    "            predictions = ml_Models.predict(X_test)\n",
    "            errors = abs(predictions - y_test)\n",
    "            MAPE = 100 * np.mean((errors / y_test))\n",
    "            NMBE = 100 * (sum(y_test - predictions) / (pd.Series(y_test).count() * np.mean(y_test)))\n",
    "            CVRSME = 100 * ((sum((y_test - predictions)**2) / (pd.Series(y_test).count()-1))**(0.5)) / np.mean(y_test)\n",
    "            RSQUARED = r2_score(y_test, predictions)\n",
    "            \n",
    "            index+=1\n",
    "            if (buildingindex==1):\n",
    "#               create the csv at the start of looping each metrics for each building\n",
    "                temporary = pd.DataFrame(columns=['buildingName','MAPE','NMBE','CVRSME','RSQUARED',\n",
    "                                                  'trainedMonths_'+str(months_for_train),'testMonths_'+str(months_for_test)])\n",
    "                temporary.to_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/' \n",
    "                                 + ml_Models_names + '_metrics_cross_validation_' + str(index) + '.csv', index=False)\n",
    "#           read and the csv and metrics result\n",
    "            metrics_prev = pd.read_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/' \n",
    "                                 + ml_Models_names + '_metrics_cross_validation_' + str(index) + '.csv')\n",
    "            df = pd.DataFrame([[single_building, MAPE, NMBE, CVRSME, RSQUARED]],\n",
    "                              columns=['buildingName','MAPE','NMBE','CVRSME','RSQUARED'])\n",
    "#           write the csv\n",
    "            metrics = pd.concat([df, metrics_prev], sort=False)\n",
    "#           export csv\n",
    "            metrics.to_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/' \n",
    "                                 + ml_Models_names + '_metrics_cross_validation_' + str(index) + '.csv', index=False)\n",
    "\n",
    "    \n",
    "ml_Models_lists = [['RandomForestRegressor_FE', RandomForestRegressor(n_estimators=100, random_state=42)]]\n",
    "weatherPoints = ['TemperatureC', 'Humidity','Dew PointC','Sea Level PressurehPa', \n",
    "                 'Wind Direction','Conditions','WindDirDegrees']\n",
    "n_timeSeriesSplits = 3\n",
    "buildingNames = meta.dropna(subset=['annualschedule']).index[100:101] #drop buildings with missing schedule\n",
    "agg_primitives = []\n",
    "trans_primitives = ['weekday','hour','is_weekend','divide_numeric',\n",
    "                               'and','multiply_numeric','divide_by_feature','absolute','week','subtract_numeric',\n",
    "                               'percentile']\n",
    "varianceThreshold = 0.8\n",
    "SelectPercentile_num = 5\n",
    "SelectPercentile_func = f_regression\n",
    "RFE_step = 10\n",
    "\n",
    "for elem in ml_Models_lists:\n",
    "#     ml_Models_names = elem[0], ml_Models = elem[1], not sure why this gives warning 'no n_estimator'\n",
    "    loopModels_and_Metrics(ml_Models_names = elem[0], ml_Models=elem[1],weatherPoints=weatherPoints,\n",
    "                           buildingNames=buildingNames, n_timeSeriesSplits=n_timeSeriesSplits, \n",
    "                           agg_primitives=agg_primitives, trans_primitives=trans_primitives,varianceThreshold=varianceThreshold,\n",
    "                           SelectPercentile_num=SelectPercentile_num, SelectPercentile_func=SelectPercentile_func, RFE_step=RFE_step)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start \n",
    "print('Time per building after FE and FS:'+ time.strftime(\"%H:%M:%S\", time.gmtime(elapsed)))\n",
    "\n",
    "\n",
    "\n",
    "# all_weather_point\n",
    "# schedule_data\n",
    "# single_building_energy\n",
    "# train_test_list\n",
    "# X_train,y_train\n",
    "# X_train.shape,y_train.shape\n",
    "# X_test,y_test\n",
    "# X_test.shape,y_test.shape\n",
    "# buildingNames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
