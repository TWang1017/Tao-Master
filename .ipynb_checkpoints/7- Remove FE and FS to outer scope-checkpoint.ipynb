{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score\n",
    "import featuretools as ft\n",
    "import time\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "from sklearn.feature_selection import RFE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RandomForestRegressor_FE\n",
      "_____________\n",
      "Modelling:Office_Mark\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 98 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 68 features.\n",
      "[5 6 7] (2208, 64) (2208,) [ 8  9 10] (2208, 64) (2208,)\n",
      "[ 5  6  7  8  9 10] (4416, 64) (4416,) [11 12  1] (2209, 64) (2209,)\n",
      "[ 5  6  7  8  9 10 11 12  1] (6625, 64) (6625,) [2 3 4] (2135, 64) (2135,)\n",
      "[ 5  6  7  9 10 11  1  2  3] (6552, 64) (6552,) [ 8 12  4] (2208, 64) (2208,)\n",
      "Time per building after FE and FS:00:07:16\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Read meta data\n",
    "meta = pd.read_csv('/Users/t.wang/Desktop/Dissertation/Python/input/meta_open.csv', \n",
    "                   index_col='uid', parse_dates=['dataend','datastart'], dayfirst=True)#The data will be messed up withou specifying dayfirst\n",
    "\n",
    "\n",
    "# Read energy data\n",
    "temporal = pd.read_csv('/Users/t.wang/Desktop/Dissertation/Python/input/temp_open_utc_complete.csv', \n",
    "                   index_col='timestamp', parse_dates=True)#.tz_localize('utc')\n",
    "\n",
    "def loopModels_and_Metrics(ml_Models_names, ml_Models, weatherPoints, n_timeSeriesSplits, \n",
    "                           buildingNames, agg_primitives, trans_primitives, varianceThreshold, SelectPercentile_num,\n",
    "                           SelectPercentile_func, RFE_step):  \n",
    "    print('\\n\\n' + ml_Models_names + '\\n_____________')\n",
    "    buildingindex = 0\n",
    "    for single_building in buildingNames:\n",
    "        buildingindex+=1\n",
    "        print('Modelling:' + single_building)\n",
    "        \n",
    "        # Read energy data for each given buildingname\n",
    "        single_timezone = meta.T[single_building].timezone\n",
    "        startdate = meta.T[single_building].datastart\n",
    "        enddate = meta.T[single_building].dataend\n",
    "        single_building_energy = temporal[single_building].tz_convert(single_timezone).truncate(before = startdate, \n",
    "                                                            after = enddate)#.fillna(method='bfill').fillna(method='ffill')\n",
    "                                                            # single_building_energy, some missing data\n",
    "\n",
    "\n",
    "        # Get weather data for given building\n",
    "        weatherfile_name = meta.T[single_building].newweatherfilename\n",
    "        weather_data = pd.read_csv(os.path.join('/Users/t.wang/Desktop/Dissertation/Python/input/',\n",
    "                                                weatherfile_name),index_col='timestamp', parse_dates=True, na_values='-9999')\n",
    "        weather_data = weather_data.tz_localize(single_timezone, ambiguous = 'infer')\n",
    "        weather_point_list=[]\n",
    "        for point in weatherPoints:\n",
    "            point_data = weather_data[[point]]\n",
    "            weather_point_list.append(point_data)\n",
    "            all_weather_point = pd.concat(weather_point_list,axis=1) #axis=1, rowwise concat\n",
    "            all_weather_point = all_weather_point[~all_weather_point.index.duplicated()]#To get rid of duplicated index\n",
    "            all_weather_point = all_weather_point.reindex(pd.DatetimeIndex(start = all_weather_point.index[0], \n",
    "                                                                           periods=len(single_building_energy), \n",
    "                                                                           freq='H')).fillna(method='ffill').fillna(method='bfill')\n",
    "#             in some cases, there are more than 1 data in the same hour, creating more than 8760 points\n",
    "#             to make them consistent, take the first index minuits, based on the number of energy data,\n",
    "#             transform them into hourly data. Then we get the same number of energy data (mostly8760)\n",
    "#             DatatimeIndex them, reindex then is able to match and select those hour with the minuites\n",
    "#             same as first index, regulating the data to be consistent with number of energy points, get\n",
    "#             rid of the repeated weather data in the same hour.\n",
    "    \n",
    "        # Get schedule data for given building\n",
    "        schedule_name = meta.T[single_building].annualschedule\n",
    "        schedule_data = pd.read_csv(os.path.join('/Users/t.wang/Desktop/Dissertation/Python/input/',\n",
    "                                                schedule_name),index_col=0, header=None, parse_dates=True)\n",
    "        schedule_data = schedule_data.tz_localize(single_timezone, ambiguous = 'infer')\n",
    "        schedule_data.columns = ['seasonal']\n",
    "        schedule_data = schedule_data.reindex(pd.DatetimeIndex(start = schedule_data.index[0], periods=len(single_building_energy), \n",
    "                                                               freq='H')).fillna(method='ffill').fillna(method='bfill')\n",
    "#         same trick is applied to selecting schedule data\n",
    "\n",
    "\n",
    "        \n",
    "        features = pd.merge(pd.DataFrame(single_building_energy.index.tz_localize(None)), \n",
    "                    schedule_data.reset_index(drop=True), right_index=True, left_index=True)#remove the time zone information\n",
    "                #Map the schedule, otherwise the TimeSplits will not be able to capture all schedules, resulting in inconsistency of traning/test feature dimensions\n",
    "        features['seasonal_num'] = features.seasonal.map({'Break':0, 'Regular':1, 'Holiday':2, 'Summer':3})\n",
    "        features = features.drop('seasonal', axis=1)\n",
    "        features = pd.concat([features, all_weather_point.reset_index(drop=True)], axis=1) #.reset_index(drop=True) to get rid of the time index, otherwise two sets data will stratify\n",
    "#                 features = features.fillna(method='ffill').fillna(method='bfill')\n",
    "        # features = np.array(features)\n",
    "        labels = single_building_energy.values\n",
    "        '''FeatureTool'''\n",
    "        es = ft.EntitySet(id = 'buildingFeatures') #create Entity set\n",
    "        # create an entity from feature table, unique index is created\n",
    "        es = es.entity_from_dataframe(entity_id='featureData', dataframe=features,\n",
    "                      make_index=True, index='feature_id', time_index = 'timestamp')\n",
    "        \n",
    "\n",
    "        features_FE, feature_names = ft.dfs(entityset = es, target_entity = 'featureData', max_depth = 2\n",
    "                        ,agg_primitives = agg_primitives,\n",
    "                        trans_primitives = trans_primitives, verbose = False, n_jobs=1) #Not sure why n_jobs more than 1 is not working\n",
    "\n",
    "        # one hot encoding for categorical data\n",
    "        features_enc, feature_names_enc = ft.encode_features(features_FE, feature_names)\n",
    "        # Replace infinity number arising after feature generation\n",
    "        features_enc = features_enc.replace(np.inf, '9999')\n",
    "        features_enc = features_enc.replace(-np.inf, '-9999')\n",
    "        features_enc = features_enc.replace([np.nan,''],0)\n",
    "#         print(features_enc)\n",
    "\n",
    "        '''Feature Selection'''\n",
    "#                 Filter methods - Removing features with low variance - SKlearn\n",
    "        y = labels\n",
    "        X = features_enc\n",
    "        sel = VarianceThreshold(threshold=(varianceThreshold * (1 - varianceThreshold)))\n",
    "        X_Variance = sel.fit_transform(X)\n",
    "\n",
    "#                 Filter methods - Univariate feature selection - SKlearn    \n",
    "        X_Variance_Percentile = SelectPercentile(score_func=SelectPercentile_func, percentile=SelectPercentile_num).fit_transform(X_Variance,y)\n",
    "#         Extract one year data for later Wrapper methods comparison\n",
    "#         pd.DataFrame(X_Variance_Percentile).to_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/'+ 'One_year_features' + '.csv', index=False)\n",
    "#         pd.DataFrame(y).to_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/'+ 'One_year_target' + '.csv', index=False)\n",
    "                                \n",
    "#                 Wrapper methods - Recursive feature elimination -SKlearn\n",
    "        Wrapper_model = ml_Models\n",
    "        rfe = RFE(estimator=Wrapper_model, step=RFE_step,verbose=True)\n",
    "        rfe = rfe.fit(X_Variance_Percentile, y)\n",
    "        X_Variance_Percentile_RFE = rfe.transform(X_Variance_Percentile)  \n",
    "#         add timestamp back to features for further Time split\n",
    "        X_Variance_Percentile_RFE = pd.DataFrame(X_Variance_Percentile_RFE).set_index(features.timestamp)\n",
    "        y = pd.DataFrame(y).set_index(features.timestamp)\n",
    "#         print(X_Variance_Percentile_RFE)\n",
    "#         print(y.shape)\n",
    "        \n",
    "                # Create TimeSeriesSplit\n",
    "        # get the month number for splitting\n",
    "        months = np.array(single_building_energy.index.month.unique())\n",
    "        tscv = TimeSeriesSplit(n_splits=n_timeSeriesSplits)\n",
    "        train_test_list = []\n",
    "        for train_index, test_index in tscv.split(months):\n",
    "            months_train, months_test = months[train_index], months[test_index]\n",
    "            train_test_list.append([months_train, months_test])\n",
    "        # Add the 'every-four-month' version, 5th month is duplicated in Clayton's notebook\n",
    "        train_test_list.append([np.concatenate([months[0:3],months[4:7],months[8:11]]),\n",
    "                                             np.array([months[3],months[7],months[11]])])\n",
    "        \n",
    "        index = 0 #index for each TimeSeries cv\n",
    "        for train_index, test_index in train_test_list: #get rid of the 'array', extract the numeric months from the list\n",
    "        #     print(train_index, test_index)\n",
    "            months_for_train = train_index\n",
    "            months_for_test = test_index\n",
    "\n",
    "            # Create features and labels under last 'for' loop such that all TimeSeriesSplit could be implenmented\n",
    "            def get_features_and_labels(train_or_test):\n",
    "                nonlocal X_Variance_Percentile_RFE\n",
    "                nonlocal y\n",
    "                X_Variance_Percentile_RFE_n = np.array(X_Variance_Percentile_RFE[X_Variance_Percentile_RFE.index.month.isin(train_or_test)])\n",
    "                y_n = np.array(y[y.index.month.isin(train_or_test)].T)[0]\n",
    "                #rename _n is required, otherwise the function will run on top of incomplete dataset after one running(after traindata, testdata disappeared)     \n",
    "#                 transform it to array for ml \n",
    "                return X_Variance_Percentile_RFE_n, y_n\n",
    "\n",
    "\n",
    "            # test on model and calculate errors\n",
    "            X_train, y_train = get_features_and_labels(train_or_test=months_for_train)\n",
    "            X_test, y_test = get_features_and_labels(train_or_test=months_for_test)\n",
    "            print(months_for_train, X_train.shape, y_train.shape, months_for_test, X_test.shape, y_test.shape)\n",
    "#             print(X_test)\n",
    "#             print(y_test)\n",
    "#             compare = pd.concat([pd.DataFrame(X_train), pd.DataFrame(X_test)], axis=1) \n",
    "#             pd.DataFrame(X_train).to_csv('/Users/t.wang/Desktop/' + 'X_train' +'.csv', index=False)\n",
    "#             pd.DataFrame(X_test).to_csv('/Users/t.wang/Desktop/' + 'X_test' +'.csv', index=False)\n",
    "            ml_Models.fit(X_train, y_train)\n",
    "            predictions = ml_Models.predict(X_test)\n",
    "            errors = abs(predictions - y_test)\n",
    "            MAPE = 100 * np.mean((errors / y_test))\n",
    "            NMBE = 100 * (sum(y_test - predictions) / (pd.Series(y_test).count() * np.mean(y_test)))\n",
    "            CVRSME = 100 * ((sum((y_test - predictions)**2) / (pd.Series(y_test).count()-1))**(0.5)) / np.mean(y_test)\n",
    "            RSQUARED = r2_score(y_test, predictions)\n",
    "            \n",
    "            index+=1\n",
    "            if (buildingindex==1):\n",
    "#               create the csv at the start of looping each metrics for each building\n",
    "                temporary = pd.DataFrame(columns=['buildingName','MAPE','NMBE','CVRSME','RSQUARED',\n",
    "                                                  'trainedMonths_'+str(months_for_train),'testMonths_'+str(months_for_test)])\n",
    "                temporary.to_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/' \n",
    "                                 + ml_Models_names + '_metrics_cross_validation_' + str(index) + '.csv', index=False)\n",
    "#           read and the csv and metrics result\n",
    "            metrics_prev = pd.read_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/' \n",
    "                                 + ml_Models_names + '_metrics_cross_validation_' + str(index) + '.csv')\n",
    "            df = pd.DataFrame([[single_building, MAPE, NMBE, CVRSME, RSQUARED]],\n",
    "                              columns=['buildingName','MAPE','NMBE','CVRSME','RSQUARED'])\n",
    "#           write the csv\n",
    "            metrics = pd.concat([df, metrics_prev], sort=False)\n",
    "#           export csv\n",
    "            metrics.to_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/' \n",
    "                                 + ml_Models_names + '_metrics_cross_validation_' + str(index) + '.csv', index=False)\n",
    "\n",
    "    \n",
    "ml_Models_lists = [['RandomForestRegressor_FE', RandomForestRegressor(n_estimators=100, random_state=42)]]\n",
    "weatherPoints = ['TemperatureC', 'Humidity','Dew PointC','Sea Level PressurehPa', \n",
    "                 'Wind Direction','Conditions','WindDirDegrees']\n",
    "n_timeSeriesSplits = 3\n",
    "buildingNames = meta.dropna(subset=['annualschedule']).index[100:101] #drop buildings with missing schedule\n",
    "agg_primitives = []\n",
    "trans_primitives = ['weekday','hour','is_weekend','divide_numeric',\n",
    "                               'and','multiply_numeric','divide_by_feature','absolute','week','subtract_numeric',\n",
    "                               'percentile']\n",
    "varianceThreshold = 0.8\n",
    "SelectPercentile_num = 5\n",
    "SelectPercentile_func = f_regression\n",
    "RFE_step = 10\n",
    "\n",
    "for elem in ml_Models_lists:\n",
    "#     ml_Models_names = elem[0], ml_Models = elem[1], not sure why this gives warning 'no n_estimator'\n",
    "    loopModels_and_Metrics(ml_Models_names = elem[0], ml_Models=elem[1],weatherPoints=weatherPoints,\n",
    "                           buildingNames=buildingNames, n_timeSeriesSplits=n_timeSeriesSplits, \n",
    "                           agg_primitives=agg_primitives, trans_primitives=trans_primitives,varianceThreshold=varianceThreshold,\n",
    "                           SelectPercentile_num=SelectPercentile_num, SelectPercentile_func=SelectPercentile_func, RFE_step=RFE_step)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start \n",
    "print('Time per building after FE and FS:'+ time.strftime(\"%H:%M:%S\", time.gmtime(elapsed)))\n",
    "\n",
    "\n",
    "\n",
    "# all_weather_point\n",
    "# schedule_data\n",
    "# single_building_energy\n",
    "# train_test_list\n",
    "# X_train,y_train\n",
    "# X_train.shape,y_train.shape\n",
    "# X_test,y_test\n",
    "# X_test.shape,y_test.shape\n",
    "# buildingNames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
