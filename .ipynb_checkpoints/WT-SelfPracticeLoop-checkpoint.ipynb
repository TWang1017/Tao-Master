{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RandomForestRegressor\n",
      "_____________\n",
      "Modelling:Office_Abbey\n",
      "Modelling:Office_Abigail\n",
      "Modelling:Office_Al\n",
      "Modelling:Office_Alannah\n",
      "Modelling:Office_Aliyah\n",
      "Modelling:Office_Allyson\n",
      "Modelling:Office_Alyson\n",
      "Modelling:Office_Amelia\n",
      "Modelling:Office_Amelie\n",
      "Modelling:Office_Anastasia\n",
      "Modelling:Office_Andrea\n",
      "Modelling:Office_Angelica\n",
      "Modelling:Office_Angelina\n",
      "Modelling:Office_Angelo\n",
      "Modelling:Office_Annika\n",
      "Modelling:Office_Ashanti\n",
      "Modelling:Office_Asher\n",
      "Modelling:Office_Aubrey\n",
      "Modelling:Office_Autumn\n",
      "Modelling:Office_Ava\n",
      "Modelling:Office_Ayden\n",
      "Modelling:Office_Ayesha\n",
      "Modelling:Office_Benjamin\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of labels=2179 does not match number of samples=2184",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-52f040511551>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;31m#     ml_Models_names = elem[0], ml_Models = elem[1], not sure why this gives warning 'no n_estimator'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     loopModels_and_Metrics(ml_Models_names = elem[0], ml_Models=elem[1],weatherPoints=weatherPoints,\n\u001b[0;32m--> 143\u001b[0;31m                        buildingNames=buildingNames, n_timeSeriesSplits=n_timeSeriesSplits)\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;31m# all_weather_point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m# schedule_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-52f040511551>\u001b[0m in \u001b[0;36mloopModels_and_Metrics\u001b[0;34m(ml_Models_names, ml_Models, weatherPoints, n_timeSeriesSplits, buildingNames)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m#             print(months_for_train, X_train.shape, y_train.shape, months_for_test, X_test.shape, y_test.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m#             print(X_train, X_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mml_Models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml_Models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 330\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             raise ValueError(\"Number of labels=%d does not match \"\n\u001b[0;32m--> 250\u001b[0;31m                              \"number of samples=%d\" % (len(y), n_samples))\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min_weight_fraction_leaf must in [0, 0.5]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels=2179 does not match number of samples=2184"
     ]
    }
   ],
   "source": [
    "# Read meta data\n",
    "meta = pd.read_csv('/Users/t.wang/Desktop/Dissertation/Python/input/meta_open.csv', \n",
    "                   index_col='uid', parse_dates=['dataend','datastart'], dayfirst=True)#The data will be messed up withou specifying dayfirst\n",
    "\n",
    "\n",
    "# Read energy data\n",
    "temporal = pd.read_csv('/Users/t.wang/Desktop/Dissertation/Python/input/temp_open_utc_complete.csv', \n",
    "                   index_col='timestamp', parse_dates=True)\n",
    "\n",
    "def loopModels_and_Metrics(ml_Models_names, ml_Models, weatherPoints, n_timeSeriesSplits, buildingNames):  \n",
    "    print('\\n\\n' + ml_Models_names + '\\n_____________')\n",
    "    buildingindex = 0\n",
    "    for single_building in buildingNames:\n",
    "        buildingindex+=1\n",
    "        print('Modelling:' + single_building)\n",
    "        \n",
    "        # Read energy data for each given buildingname\n",
    "        startdate = meta.datastart[single_building]\n",
    "        enddate = meta.dataend[single_building]\n",
    "        single_building_energy = temporal[single_building].truncate(before = startdate, \n",
    "                                                            after = enddate).fillna(method='bfill').fillna(method='ffill')\n",
    "                                                            # single_building_energy, some missing data\n",
    "\n",
    "\n",
    "        # Get weather data for given building\n",
    "        weatherfile_name = meta.newweatherfilename[single_building]\n",
    "        weather_data = pd.read_csv(os.path.join('/Users/t.wang/Desktop/Dissertation/Python/input/',\n",
    "                                                weatherfile_name),index_col='timestamp', parse_dates=True)\n",
    "        weather_point_list=[]\n",
    "        for point in weatherPoints:\n",
    "            point_data = weather_data[[point]]\n",
    "            weather_point_list.append(point_data)\n",
    "            all_weather_point = pd.concat(weather_point_list,axis=1) #axis=1, rowwise concat\n",
    "            all_weather_point = all_weather_point[~all_weather_point.index.duplicated()]#To get rid of duplicated index\n",
    "            all_weather_point = all_weather_point.reindex(pd.DatetimeIndex(start = all_weather_point.index[0], \n",
    "                                                                           periods=len(single_building_energy), \n",
    "                                                                           freq='H')).fillna(method='ffill').fillna(method='bfill')\n",
    "#             in some cases, there are more than 1 data in the same hour, creating more than 8760 points\n",
    "#             to make them consistent, take the first index minuits, based on the number of energy data,\n",
    "#             transform them into hourly data. Then we get the same number of energy data (mostly8760)\n",
    "#             DatatimeIndex them, reindex then is able to match and select those hour with the minuites\n",
    "#             same as first index, regulating the data to be consistent with number of energy points, get\n",
    "#             rid of the repeated weather data in the same hour.\n",
    "    \n",
    "        # Get schedule data for given building\n",
    "        schedule_name = meta.annualschedule[single_building]\n",
    "        schedule_data = pd.read_csv(os.path.join('/Users/t.wang/Desktop/Dissertation/Python/input/',\n",
    "                                                schedule_name),index_col=0, header=None, parse_dates=True)\n",
    "        schedule_data.columns = ['seasonal']\n",
    "        schedule_data = schedule_data.reindex(pd.DatetimeIndex(start = schedule_data.index[0], periods=len(single_building_energy), \n",
    "                                                               freq='H')).fillna(method='ffill').fillna(method='bfill')\n",
    "#         same trick is applied to selecting schedule data\n",
    "\n",
    "        # Create TimeSeriesSplit\n",
    "        # get the month number for splitting\n",
    "        months = np.array(single_building_energy.index.month.unique())\n",
    "        tscv = TimeSeriesSplit(n_splits=n_timeSeriesSplits)\n",
    "        train_test_list = []\n",
    "        for train_index, test_index in tscv.split(months):\n",
    "            months_train, months_test = months[train_index], months[test_index]\n",
    "            train_test_list.append([months_train, months_test])\n",
    "        # Add the 'every-four-month' version, 5th month is duplicated in Clayton's notebook\n",
    "        train_test_list.append([np.concatenate([months[0:3],months[4:7],months[8:11]]),\n",
    "                                             np.array([months[3],months[7],months[11]])])\n",
    "        \n",
    "        index = 0 #index for each TimeSeries cv\n",
    "        for train_index, test_index in train_test_list: #get rid of the 'array', extract the numeric months from the list\n",
    "        #     print(train_index, test_index)\n",
    "            months_for_train = train_index\n",
    "            months_for_test = test_index\n",
    "\n",
    "            # Create features and labels under last 'for' loop such that all TimeSeriesSplit could be implenmented\n",
    "            def get_features_and_labels(train_or_test):\n",
    "                nonlocal single_building_energy #nonlocal means: \"look for this variable in the outer scope\"\n",
    "                nonlocal all_weather_point\n",
    "                nonlocal schedule_data\n",
    "                single_building_energy_n = single_building_energy[single_building_energy.index.month.isin(train_or_test)]\n",
    "                all_weather_point_n = all_weather_point[all_weather_point.index.month.isin(train_or_test)]\n",
    "                schedule_data_n = schedule_data[schedule_data.index.month.isin(train_or_test)]\n",
    "                #rename _n is required, otherwise the function will run on top of incomplete dataset after one running(after traindata, testdata disappeared)\n",
    "                \n",
    "                \n",
    "                '''Issues here, the shape of energy, weather and schedule is differet after above code, \n",
    "                resulting in inconsistent samples and lables'''\n",
    "\n",
    "                \n",
    "                features = pd.merge(pd.get_dummies(single_building_energy_n.index.hour),\n",
    "                                     pd.get_dummies(single_building_energy_n.index.dayofweek), right_index=True, left_index=True)\n",
    "#                 features = pd.merge(features, pd.get_dummies(schedule_data_n.reset_index(drop=True)), right_index=True, left_index=True)\n",
    "                features = pd.merge(features, schedule_data_n.reset_index(drop=True), right_index=True, left_index=True)\n",
    "                features['seasonal_num'] = features.seasonal.map({'Break':0, 'Regular':1, 'Holiday':2, 'Summer':3})\n",
    "                features = features.drop('seasonal', axis=1)\n",
    "        #     instead of get dummies in schedule data, map schedule strings to numbers in just one column helps to solve the inconsistency of schedules\n",
    "        #     may result in information loss\n",
    "                features = pd.concat([features, all_weather_point_n.reset_index(drop=True)], axis=1) \n",
    "                #.reset_index(drop=True) to get rid of the time index, otherwise two sets data will stratify\n",
    "                features = features.fillna(method='ffill').fillna(method='bfill')\n",
    "                features = np.array(features)\n",
    "                labels = single_building_energy_n.values\n",
    "                return features, labels\n",
    "\n",
    "\n",
    "            # test on model and calculate errors\n",
    "            X_train, y_train = get_features_and_labels(train_or_test=months_for_train)\n",
    "            X_test, y_test = get_features_and_labels(train_or_test=months_for_test)\n",
    "#             print(months_for_train, X_train.shape, y_train.shape, months_for_test, X_test.shape, y_test.shape)\n",
    "#             print(X_train, X_test)\n",
    "            ml_Models.fit(X_train, y_train)\n",
    "            predictions = ml_Models.predict(X_test)\n",
    "            errors = abs(predictions - y_test)\n",
    "            MAPE = 100 * np.mean((errors / y_test))\n",
    "            NMBE = 100 * (sum(y_test - predictions) / (pd.Series(y_test).count() * np.mean(y_test)))\n",
    "            CVRSME = 100 * ((sum((y_test - predictions)**2) / (pd.Series(y_test).count()-1))**(0.5)) / np.mean(y_test)\n",
    "            RSQUARED = r2_score(y_test, predictions)\n",
    "            \n",
    "            index+=1\n",
    "            if (buildingindex==1):\n",
    "#               create the csv at the start of looping each metrics for each building\n",
    "                temporary = pd.DataFrame(columns=['buildingName','MAPE','NMBE','CVRSME','RSQUARED',\n",
    "                                                  'trainedMonths_'+str(months_for_train),'testMonths_'+str(months_for_test)])\n",
    "                temporary.to_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/' \n",
    "                                 + ml_Models_names + '_metrics_cross_validation_' + str(index) + '.csv', index=False)\n",
    "#           read and the csv and metrics result\n",
    "            metrics_prev = pd.read_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/' \n",
    "                                 + ml_Models_names + '_metrics_cross_validation_' + str(index) + '.csv')\n",
    "            df = pd.DataFrame([[single_building, MAPE, NMBE, CVRSME, RSQUARED]],\n",
    "                              columns=['buildingName','MAPE','NMBE','CVRSME','RSQUARED'])\n",
    "#           write the csv\n",
    "            metrics = pd.concat([df, metrics_prev], sort=False)\n",
    "#           export csv\n",
    "            metrics.to_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/' \n",
    "                                 + ml_Models_names + '_metrics_cross_validation_' + str(index) + '.csv', index=False)\n",
    "\n",
    "    \n",
    "ml_Models_lists = [['RandomForestRegressor', RandomForestRegressor(n_estimators=100, random_state=42)]]\n",
    "weatherPoints = ['TemperatureC', 'Humidity']\n",
    "n_timeSeriesSplits = 3\n",
    "buildingNames = meta.dropna(subset=['annualschedule']).index #drop buildings with missing schedule\n",
    "\n",
    "for elem in ml_Models_lists:\n",
    "#     ml_Models_names = elem[0], ml_Models = elem[1], not sure why this gives warning 'no n_estimator'\n",
    "    loopModels_and_Metrics(ml_Models_names = elem[0], ml_Models=elem[1],weatherPoints=weatherPoints,\n",
    "                       buildingNames=buildingNames, n_timeSeriesSplits=n_timeSeriesSplits)\n",
    "# all_weather_point\n",
    "# schedule_data\n",
    "# single_building_energy\n",
    "# train_test_list\n",
    "# X_train,y_train\n",
    "# X_train.shape,y_train.shape\n",
    "# X_test,y_test\n",
    "# X_test.shape,y_test.shape\n",
    "# buildingNames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
