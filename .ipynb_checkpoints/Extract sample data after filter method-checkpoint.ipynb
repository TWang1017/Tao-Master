{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score\n",
    "import featuretools as ft\n",
    "from feature_selector import FeatureSelector\n",
    "import time\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
    "from sklearn.feature_selection import RFE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RandomForestRegressor_FE\n",
      "_____________\n",
      "Modelling:Office_Abigail\n",
      "Built 41 features\n",
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "15 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 15 features including one-hot features.\n",
      "Modelling:Office_Al\n",
      "Built 41 features\n",
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "15 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 15 features including one-hot features.\n",
      "Modelling:Office_Alannah\n",
      "Built 41 features\n",
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "15 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 15 features including one-hot features.\n",
      "Modelling:PrimClass_Jamie\n",
      "Built 41 features\n",
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "8 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 8 features including one-hot features.\n",
      "Modelling:PrimClass_Jane\n",
      "Built 41 features\n",
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "8 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 8 features including one-hot features.\n",
      "Modelling:PrimClass_Janelle\n",
      "Built 41 features\n",
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "8 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 8 features including one-hot features.\n",
      "Modelling:UnivClass_Craig\n",
      "Built 41 features\n",
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "13 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 13 features including one-hot features.\n",
      "Modelling:UnivClass_Jadon\n",
      "Built 41 features\n",
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "12 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 12 features including one-hot features.\n",
      "Modelling:UnivClass_Maddison\n",
      "Built 41 features\n",
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "13 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 13 features including one-hot features.\n",
      "Modelling:UnivDorm_Leann\n",
      "Built 41 features\n",
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "13 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 13 features including one-hot features.\n",
      "Modelling:UnivDorm_Leonard\n",
      "Built 41 features\n",
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "13 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 13 features including one-hot features.\n",
      "Modelling:UnivDorm_Leslie\n",
      "Built 41 features\n",
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "13 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 13 features including one-hot features.\n",
      "Modelling:UnivLab_Lester\n",
      "Built 41 features\n",
      "Elapsed: 00:01 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "13 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 13 features including one-hot features.\n",
      "Modelling:UnivLab_Levi\n",
      "Built 41 features\n",
      "Elapsed: 00:01 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "13 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 13 features including one-hot features.\n",
      "Modelling:UnivLab_Lilly\n",
      "Built 41 features\n",
      "Elapsed: 00:01 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 10/10 chunks\n",
      "13 features with a correlation magnitude greater than 0.50.\n",
      "\n",
      "Data has not been one-hot encoded\n",
      "Removed 13 features including one-hot features.\n",
      "Time per building after FE and FS:00:02:16\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Read meta data\n",
    "meta = pd.read_csv('/Users/t.wang/Desktop/Dissertation/Python/input/meta_open.csv', \n",
    "                   index_col='uid', parse_dates=['dataend','datastart'], dayfirst=True)#The data will be messed up withou specifying dayfirst\n",
    "\n",
    "\n",
    "# Read energy data\n",
    "temporal = pd.read_csv('/Users/t.wang/Desktop/Dissertation/Python/input/temp_open_utc_complete.csv', \n",
    "                   index_col='timestamp', parse_dates=True)#.tz_localize('utc')\n",
    "\n",
    "def loopModels_and_Metrics(ml_Models_names, ml_Models, weatherPoints, cor_threshold, \n",
    "                           buildingNames, agg_primitives, trans_primitives, varianceThreshold, SelectPercentile_num,\n",
    "                           SelectPercentile_func, RFE_step):  \n",
    "    print('\\n\\n' + ml_Models_names + '\\n_____________')\n",
    "    buildingindex = 0\n",
    "    for single_building in buildingNames:\n",
    "        buildingindex+=1\n",
    "        print('Modelling:' + single_building)\n",
    "        \n",
    "        # Read energy data for each given buildingname\n",
    "        single_timezone = meta.T[single_building].timezone\n",
    "        startdate = meta.T[single_building].datastart\n",
    "        enddate = meta.T[single_building].dataend\n",
    "        single_building_energy = temporal[single_building].tz_convert(single_timezone).truncate(before = startdate, \n",
    "                                                            after = enddate)#.fillna(method='bfill').fillna(method='ffill')\n",
    "                                                            # single_building_energy, some missing data\n",
    "\n",
    "\n",
    "        # Get weather data for given building\n",
    "        weatherfile_name = meta.T[single_building].newweatherfilename\n",
    "        weather_data = pd.read_csv(os.path.join('/Users/t.wang/Desktop/Dissertation/Python/input/',\n",
    "                                                weatherfile_name),index_col='timestamp', parse_dates=True, na_values='-9999')\n",
    "        weather_data = weather_data.tz_localize(single_timezone, ambiguous = 'infer')\n",
    "        weather_point_list=[]\n",
    "        for point in weatherPoints:\n",
    "            point_data = weather_data[[point]]\n",
    "            weather_point_list.append(point_data)\n",
    "            all_weather_point = pd.concat(weather_point_list,axis=1) #axis=1, rowwise concat\n",
    "            all_weather_point = all_weather_point[~all_weather_point.index.duplicated()]#To get rid of duplicated index\n",
    "            all_weather_point = all_weather_point.reindex(pd.DatetimeIndex(start = all_weather_point.index[0], \n",
    "                                                                           periods=len(single_building_energy), \n",
    "                                                                           freq='H')).fillna(method='ffill').fillna(method='bfill')\n",
    "#             in some cases, there are more than 1 data in the same hour, creating more than 8760 points\n",
    "#             to make them consistent, take the first index minuits, based on the number of energy data,\n",
    "#             transform them into hourly data. Then we get the same number of energy data (mostly8760)\n",
    "#             DatatimeIndex them, reindex then is able to match and select those hour with the minuites\n",
    "#             same as first index, regulating the data to be consistent with number of energy points, get\n",
    "#             rid of the repeated weather data in the same hour.\n",
    "    \n",
    "        # Get schedule data for given building\n",
    "        schedule_name = meta.T[single_building].annualschedule\n",
    "        schedule_data = pd.read_csv(os.path.join('/Users/t.wang/Desktop/Dissertation/Python/input/',\n",
    "                                                schedule_name),index_col=0, header=None, parse_dates=True)\n",
    "        schedule_data = schedule_data.tz_localize(single_timezone, ambiguous = 'infer')\n",
    "        schedule_data.columns = ['seasonal']\n",
    "        schedule_data = schedule_data.reindex(pd.DatetimeIndex(start = schedule_data.index[0], periods=len(single_building_energy), \n",
    "                                                               freq='H')).fillna(method='ffill').fillna(method='bfill')\n",
    "#         same trick is applied to selecting schedule data\n",
    "\n",
    "\n",
    "        \n",
    "        features = pd.merge(pd.DataFrame(single_building_energy.index.tz_localize(None)), \n",
    "                    schedule_data.reset_index(drop=True), right_index=True, left_index=True)#remove the time zone information\n",
    "                #Map the schedule, otherwise the TimeSplits will not be able to capture all schedules, resulting in inconsistency of traning/test feature dimensions\n",
    "        features['seasonal_num'] = features.seasonal.map({'Break':0, 'Regular':1, 'Holiday':2, 'Summer':3})\n",
    "        features = features.drop('seasonal', axis=1)\n",
    "        features = pd.concat([features, all_weather_point.reset_index(drop=True)], axis=1) #.reset_index(drop=True) to get rid of the time index, otherwise two sets data will stratify\n",
    "#                 features = features.fillna(method='ffill').fillna(method='bfill')\n",
    "        # features = np.array(features)\n",
    "        labels = single_building_energy.values\n",
    "        '''FeatureTool'''\n",
    "        es = ft.EntitySet(id = 'buildingFeatures') #create Entity set\n",
    "        # create an entity from feature table, unique index is created\n",
    "        es = es.entity_from_dataframe(entity_id='featureData', dataframe=features,\n",
    "                      make_index=True, index='feature_id', time_index = 'timestamp')\n",
    "\n",
    "        features_FE, feature_names = ft.dfs(entityset = es, target_entity = 'featureData', max_depth = 2\n",
    "                        ,agg_primitives = agg_primitives,\n",
    "                        trans_primitives = trans_primitives, verbose = True, n_jobs=1) #Not sure why n_jobs more than 1 is not working\n",
    "\n",
    "        # one hot encoding for categorical data\n",
    "        features_enc, feature_names_enc = ft.encode_features(features_FE, feature_names)\n",
    "        # Replace infinity number arising after feature generation\n",
    "        features_enc = features_enc.replace(np.inf, '9999')\n",
    "        features_enc = features_enc.replace(-np.inf, '-9999')\n",
    "        features_enc = features_enc.replace([np.nan,''],0)\n",
    "#         print(features_enc)\n",
    "\n",
    "        '''Feature Selection'''\n",
    "#                 Filter methods - Remove collinear features - FeatureSelector\n",
    "        y = labels\n",
    "        X = features_enc\n",
    "        fs = FeatureSelector(data = X, labels = y)\n",
    "        fs.identify_collinear(correlation_threshold = cor_threshold, one_hot=False)\n",
    "        X_collinear = fs.remove(methods = ['collinear'], keep_one_hot=False)\n",
    "        \n",
    "#         Filter methods - Remove features with low variance - SKlearn\n",
    "        sel = VarianceThreshold(threshold=(varianceThreshold * (1 - varianceThreshold)))\n",
    "        X_collinear_Variance = sel.fit_transform(X_collinear)\n",
    "        X_collinear_Variance = pd.DataFrame(X_collinear_Variance)\n",
    "\n",
    "#                 Filter methods - SelectPercentile based on mutual information - SKlearn   \n",
    "        X_collinear_Variance_MI = SelectPercentile(score_func=SelectPercentile_func, percentile=SelectPercentile_num).fit_transform(X_collinear_Variance,y)\n",
    "#         Extract one year data for later Wrapper methods comparison\n",
    "        pd.DataFrame(X_collinear_Variance_MI).to_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/One_year_data/'+ str(single_building)+'_One_year_features' + '.csv', index=False)\n",
    "        pd.DataFrame(y).to_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/One_year_data/'+ str(single_building)+'_One_year_target' + '.csv', index=False)\n",
    "                                \n",
    "\n",
    "    \n",
    "ml_Models_lists = [['RandomForestRegressor_FE', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)]]\n",
    "weatherPoints = ['TemperatureC', 'Humidity','Dew PointC','Sea Level PressurehPa', \n",
    "                 'Wind Direction','Conditions','WindDirDegrees']\n",
    "\n",
    "\n",
    "chosen_buildings = [1,2,3,165,166,167,288,289,290,360,361,362,450,451,452]\n",
    "#drop buildings with missing schedule\n",
    "buildingNames = meta.dropna(subset=['annualschedule']).index[chosen_buildings]\n",
    "    \n",
    "agg_primitives = []\n",
    "trans_primitives = ['weekday','hour','is_weekend','divide_numeric',\n",
    "                               'and','multiply_numeric','divide_by_feature','absolute','week','subtract_numeric',\n",
    "                               'percentile']\n",
    "cor_threshold = 0.5\n",
    "varianceThreshold = 0.8\n",
    "SelectPercentile_num = 5\n",
    "SelectPercentile_func = mutual_info_regression\n",
    "RFE_step = 10\n",
    "\n",
    "for elem in ml_Models_lists:\n",
    "#     ml_Models_names = elem[0], ml_Models = elem[1], not sure why this gives warning 'no n_estimator'\n",
    "    loopModels_and_Metrics(ml_Models_names = elem[0], ml_Models=elem[1],weatherPoints=weatherPoints,\n",
    "                           buildingNames=buildingNames, cor_threshold = cor_threshold,\n",
    "                           agg_primitives=agg_primitives, trans_primitives=trans_primitives,varianceThreshold=varianceThreshold,\n",
    "                           SelectPercentile_num=SelectPercentile_num, SelectPercentile_func=SelectPercentile_func, RFE_step=RFE_step)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start \n",
    "print('Time per building after FE and FS:'+ time.strftime(\"%H:%M:%S\", time.gmtime(elapsed)))\n",
    "\n",
    "\n",
    "# all_weather_point\n",
    "# schedule_data\n",
    "# single_building_energy\n",
    "# train_test_list\n",
    "# X_train,y_train\n",
    "# X_train.shape,y_train.shape\n",
    "# X_test,y_test\n",
    "# X_test.shape,y_test.shape\n",
    "# buildingNames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check csv with specified columns (practice, irrelevant to above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAPE</th>\n",
       "      <th>NMBE</th>\n",
       "      <th>CVRSME</th>\n",
       "      <th>RSQUARED</th>\n",
       "      <th>trainedMonths_</th>\n",
       "      <th>testMonths_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.571059</td>\n",
       "      <td>1.746917</td>\n",
       "      <td>16.036343</td>\n",
       "      <td>0.432561</td>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>[4 5 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.379363</td>\n",
       "      <td>5.686699</td>\n",
       "      <td>9.761787</td>\n",
       "      <td>0.592209</td>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>[4 5 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.512372</td>\n",
       "      <td>4.410711</td>\n",
       "      <td>9.115582</td>\n",
       "      <td>0.601550</td>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>[4 5 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.740142</td>\n",
       "      <td>-1.536381</td>\n",
       "      <td>10.919086</td>\n",
       "      <td>0.515439</td>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>[4 5 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.210618</td>\n",
       "      <td>-5.639330</td>\n",
       "      <td>14.834996</td>\n",
       "      <td>0.433792</td>\n",
       "      <td>[1 2 3]</td>\n",
       "      <td>[4 5 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAPE      NMBE     CVRSME  RSQUARED trainedMonths_ testMonths_\n",
       "0  11.571059  1.746917  16.036343  0.432561        [1 2 3]     [4 5 6]\n",
       "1   7.379363  5.686699   9.761787  0.592209        [1 2 3]     [4 5 6]\n",
       "2   6.512372  4.410711   9.115582  0.601550        [1 2 3]     [4 5 6]\n",
       "3   8.740142 -1.536381  10.919086  0.515439        [1 2 3]     [4 5 6]\n",
       "4  12.210618 -5.639330  14.834996  0.433792        [1 2 3]     [4 5 6]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can just read a single line using nrows=1 to get the cols and then re-read in the \n",
    "# full csv skipping the first col by slicing the column array from the first read\n",
    "\n",
    "cols = pd.read_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/RandomForestRegressor_metrics_cross_validation_1.csv', nrows=1).columns\n",
    "df=pd.read_csv('/Users/t.wang/Desktop/Dissertation/Python/WT-result/RandomForestRegressor_metrics_cross_validation_1.csv',usecols=cols[1:])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
